{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60f0c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73950725",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-12-14T16:10:08.509618Z",
     "iopub.status.busy": "2021-12-14T16:10:08.508729Z",
     "iopub.status.idle": "2021-12-14T16:10:12.674505Z",
     "shell.execute_reply": "2021-12-14T16:10:12.673910Z",
     "shell.execute_reply.started": "2021-12-14T14:47:03.821116Z"
    },
    "papermill": {
     "duration": 4.213162,
     "end_time": "2021-12-14T16:10:12.674671",
     "exception": false,
     "start_time": "2021-12-14T16:10:08.461509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import shap\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_columns', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69226d15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T16:10:12.754750Z",
     "iopub.status.busy": "2021-12-14T16:10:12.754070Z",
     "iopub.status.idle": "2021-12-14T16:10:19.491115Z",
     "shell.execute_reply": "2021-12-14T16:10:19.491682Z",
     "shell.execute_reply.started": "2021-12-14T14:47:08.090615Z"
    },
    "papermill": {
     "duration": 6.780121,
     "end_time": "2021-12-14T16:10:19.491861",
     "exception": false,
     "start_time": "2021-12-14T16:10:12.711740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data=pd.read_excel('Delivery truck trip data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f5c47",
   "metadata": {},
   "source": [
    "## Download and Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd58b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data step\n",
    "def load_data(download_link: str, data_path: OutputPath(str)):\n",
    "        \n",
    "    # install the necessary libraries\n",
    "    import os, sys, pickle, subprocess;\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"wget\"])\n",
    "    import wget\n",
    "    \n",
    "    # import libraries\n",
    "    import pandas as pd\n",
    "    \n",
    "    # create data_path directory\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    # download data\n",
    "    wget.download(download_link, f'{data_path}/Delivery truck trip data.xlsx')\n",
    "    \n",
    "    # read data\n",
    "    data = pd.read_excel(f\"{data_path}/Delivery truck trip data.xlsx\")\n",
    "\n",
    "    # Save data as a pickle file to be used by the tranform_data component.\n",
    "    with open(f'{data_path}/data', 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee1798d",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116b3d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data step\n",
    "\n",
    "def transform_data(data_path: InputPath(str), \n",
    "              data: OutputPath(str)):\n",
    "    \n",
    "    # install the necessary libraries\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','geopy'])\n",
    "    \n",
    "    # import Libraries\n",
    "    import os, pickle;\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    \n",
    "    # load data from data_path\n",
    "    with open(f'{data_path}/data', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    # 8. Change ontime colunm to 1 for ontime and 0 for delay\n",
    "    data.ontime.replace({'G':1, np.NaN:0}, inplace=True)\n",
    "\n",
    "    # 3. Replace NaN with unknown for vechicle type\n",
    "    data.vehicleType.replace(np.NaN, 'unknown', inplace=True)\n",
    "    \n",
    "    # 10. Filling NaN in Transportation distance with difference in lon/lot of origin and destination\n",
    "    from geopy import distance\n",
    "\n",
    "    geodistance_km = []\n",
    "    for row in data.itertuples(index=False):\n",
    "        geodistance_km.append(distance.distance(row.Org_lat_lon, row.Des_lat_lon).km)\n",
    "\n",
    "    data['geodistaince_km']=geodistance_km\n",
    "\n",
    "    # Replace NaN row in 'TRANSPORTATION_DISTANCE_IN_KM' with geodisatnce values\n",
    "    data.TRANSPORTATION_DISTANCE_IN_KM.fillna(data.geodistaince_km, inplace=True)\n",
    "    \n",
    "    #11. Create the expected travel time in hours\n",
    "    data['expected_travelhours']=(data.Planned_ETA-data.trip_start_date).astype('timedelta64[h]')\n",
    "\n",
    "    # There are negative travel hours. I replace them with 0 hour.\n",
    "    data.expected_travelhours[data.expected_travelhours<0]=0\n",
    "    data.expected_travelhours.sort_values()\n",
    "\n",
    "    # 6784 row can be dropped, because it looks like an outlier.\n",
    "    data.drop(index=6784,axis=0,inplace=True)\n",
    "    \n",
    "    # Based on the consideration above, I use the following the first 9 columes as input features. The last one is the target.\n",
    "    data_use = data[['Market/Regular ','OriginLocation_Code','DestinationLocation_Code',\n",
    "                 'TRANSPORTATION_DISTANCE_IN_KM','expected_travelhours','vehicleType',\n",
    "                 'customerID','supplierID','Material Shipped','ontime']]\n",
    "    # They are not many som just trop them.\n",
    "    data_use.dropna(axis=0,inplace=True)\n",
    "\n",
    "    # Fix these columns\n",
    "    data_use['OriginLocation_Code'] = data_use['OriginLocation_Code'].apply(str)\n",
    "    data_use['DestinationLocation_Code'] = data_use['DestinationLocation_Code'].apply(str)\n",
    "    data_use['supplierID'] = data_use['supplierID'].apply(str)\n",
    "\n",
    "    \n",
    "    #creating the transform_data_path\n",
    "    os.makedirs(data, exist_ok = True)\n",
    "    \n",
    "    #Save data as a pickle file to be used by the feature_engineering component.\n",
    "    with open(f'{data}/data', 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    \n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3977c767",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d39ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering step\n",
    "\n",
    "def feature_engineering(data: InputPath(str), \n",
    "            feat_eng_path: OutputPath(str)):\n",
    "    \n",
    "    # install the necessary libraries\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','scikit-learn'])\n",
    "    \n",
    "  \n",
    "    \n",
    "    # import Library\n",
    "    import os, pickle;\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # loading the data\n",
    "    with open(f'{data}/data', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        \n",
    "    # Take a copy for preproessing\n",
    "    df = data_use.copy()\n",
    "\n",
    "    # Make X and y\n",
    "    X = df.drop(columns='ontime', axis=1)\n",
    "    y = df['ontime']\n",
    "    \n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=22, stratify=y)\n",
    "\n",
    "    # creating the feat_eng_path\n",
    "    os.makedirs(feat_eng_path, exist_ok = True)\n",
    "      \n",
    "    # save the train_test_split data as a pickle file to be used by the modeling component.\n",
    "    with open(f'{feat_eng_path}/split_data', 'wb') as f:\n",
    "        pickle.dump((X_train, X_test, y_train, y_test), f)\n",
    "    \n",
    "    return(print('Done!'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ba83b7",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6da5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost modeling step\n",
    "\n",
    "def xgboost_modeling(feat_eng_path: InputPath(str), \n",
    "                     xgb_ensemble_path: OutputPath(str),\n",
    "                     mlpipeline_ui_metadata_path: OutputPath(str)):\n",
    "    \n",
    "    # install the necessary libraries\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','xgboost'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','scikit-learn'])\n",
    "    \n",
    "    # import Library\n",
    "    import os, json, pickle, joblib;\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from collections import namedtuple\n",
    "\n",
    "    #loading the split_data data\n",
    "    with open(f'{feat_eng_path}/split_data', 'rb') as f:\n",
    "        X_train, X_test, y_train, y_test = pickle.load(f)\n",
    "            \n",
    "    #creating the ensemble_path directory\n",
    "    os.makedirs(xgb_ensemble_path, exist_ok = True)\n",
    "    \n",
    "    # model initialization\n",
    "    xgb=XGBClassifier(scale_pos_weight=0.3627, \n",
    "                      max_depth=12, \n",
    "                      learning_rate=0.1043242, \n",
    "                      n_estimators=600, \n",
    "                      colsample_bylevel=0.8, \n",
    "                      reg_alpha=0.8,\n",
    "                      silent=True, \n",
    "                      metrics='auc', \n",
    "                      random_state=43)\n",
    "    \n",
    "    # fitting\n",
    "    xgb.fit(X_train,y_train,eval_set=[(X_train,y_train),(X_test, y_test)], early_stopping_rounds=50,verbose=50)\n",
    "    \n",
    "    # predict\n",
    "    xgb_pred = xgb.predict(X_test)\n",
    "    \n",
    "    #Save the predicted data as a pickle file to be used by the ensembling component.\n",
    "    with open(f'{xgb_ensemble_path}/xgb_pred', 'wb') as f:\n",
    "        pickle.dump(xgb_pred, f) \n",
    "    \n",
    "    # plot confusion_matrix\n",
    "    cm = confusion_matrix(y_test, xgb_pred)\n",
    "    vocab = list(np.unique(y_test))\n",
    "    \n",
    "    # confusion_matrix pair dataset \n",
    "    data = []\n",
    "    for target_index, target_row in enumerate(cm):\n",
    "        for predicted_index, count in enumerate(target_row):\n",
    "            data.append((vocab[target_index], vocab[predicted_index], count))\n",
    "    \n",
    "    # convert confusion_matrix pair dataset to dataframe\n",
    "    df = pd.DataFrame(data,columns=['target','predicted','count'])\n",
    "    \n",
    "    # change 'target', 'predicted' to integer strings\n",
    "    df[['target', 'predicted']] = (df[['target', 'predicted']].astype(int)).astype(str)\n",
    "    \n",
    "    # create kubeflow metric metadata for UI\n",
    "    metadata = {\n",
    "                \"outputs\": [\n",
    "                    {\n",
    "                        \"type\": \"confusion_matrix\",\n",
    "                        \"format\": \"csv\",\n",
    "                        \"schema\": [\n",
    "                            {\n",
    "                                \"name\": \"target\",\n",
    "                                \"type\": \"CATEGORY\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"name\": \"predicted\",\n",
    "                                \"type\": \"CATEGORY\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"name\": \"count\",\n",
    "                                \"type\": \"NUMBER\"\n",
    "                            }\n",
    "                        ],\n",
    "                        \"source\": df.to_csv(header=False, index=False),\n",
    "                        \"storage\": \"inline\",\n",
    "                        \"labels\": [\n",
    "                            \"0\",\n",
    "                            \"1\"\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "    \n",
    "    with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:\n",
    "        json.dump(metadata, metadata_file)\n",
    "\n",
    "    conf_m_result = namedtuple('conf_m_result', ['mlpipeline_ui_metadata'])\n",
    "    \n",
    "    return conf_m_result(json.dumps(metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2345519",
   "metadata": {},
   "source": [
    "## Create Pipeline Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5951e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create light weight components\n",
    "load_op = comp.create_component_from_func(load_data,base_image=\"python:3.7.1\")\n",
    "transform_op = comp.create_component_from_func(transform_data,base_image=\"python:3.7.1\")\n",
    "feature_eng_op = comp.create_component_from_func(feature_engineering,base_image=\"python:3.7.1\")\n",
    "xgboost_modeling_op = comp.create_component_from_func(xgboost_modeling, base_image=\"python:3.7.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb6d41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline\n",
    "@dsl.pipeline(name=\"logistics_service_analysis\", \n",
    "              description=\"Predicting Ontime Delivery.\")\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def logistics_service_analysis_pipeline(\n",
    "                             download_link: str,\n",
    "                             data_path: str,\n",
    "                             transform_data_path: str, \n",
    "                             feat_eng_data_path: str,\n",
    "                             xgb_ensemble_path:str,\n",
    "                            ):\n",
    "\n",
    "    # Create load container.\n",
    "    load_container = load_op(download_link)\n",
    "    # Create transform container.\n",
    "    transform_container = transform_op(load_container.output)\n",
    "    # Create feature engineering container.\n",
    "    feature_eng_container = feature_eng_op(transform_container.output)\n",
    "    # Create xgboost modeling container.\n",
    "    xgb_modeling_container = xgboost_modeling_op(feature_eng_container.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25be0fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create client that would enable communication with the Pipelines API server \n",
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1d7b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments\n",
    "download_link = \"https://github.com/saurabhpimpalkar8/Logistics_Analysis.git\"\n",
    "data_path = \"Delivery truck trip data\"\n",
    "transform_data_path = \"tdp\"\n",
    "feat_eng_data_path = \"feat\"                         \n",
    "xgb_ensemble_path = \"xep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0673f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = logistics_service_analysis_pipeline\n",
    "\n",
    "experiment_name = 'logistics_service_analysis_pipeline_xgb'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\n",
    "             \"download_link\": download_link,\n",
    "             \"data_path\": data_path,\n",
    "             \"transform_data_path\": transform_data_path,\n",
    "             \"feat_eng_data_path\": feat_eng_data_path,\n",
    "             \"xgb_ensemble_path\": xgb_ensemble_path,\n",
    "            }\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  \n",
    "  '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5f677d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb459c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d598585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba8715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f4313e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 57.840049,
   "end_time": "2021-12-14T16:10:42.581222",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-14T16:09:44.741173",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
