{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "!pip install --user --upgrade pip\n",
    "!pip install kfp --upgrade --user --quiet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "!pip show kfp"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install xgboost\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data step\n",
    "def load_data(download_link: str, data_path: OutputPath(str)):\n",
    "        \n",
    "    # install the necessary libraries\n",
    "    import os, sys, pickle, subprocess;\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"wget\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"openpyxl\"])\n",
    "    import wget\n",
    "    \n",
    "    # import libraries\n",
    "    import pandas as pd\n",
    "    import openpyxl\n",
    "    \n",
    "    # create data_path directory\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    # download data\n",
    "    wget.download(download_link, f'{data_path}/Delivery_truck_trip_data.xlsx')\n",
    "    \n",
    "    # read data\n",
    "    data = pd.read_excel(f'{data_path}/Delivery_truck_trip_data.xlsx', engine='openpyxl')\n",
    "\n",
    "    # Save data as a pickle file to be used by the tranform_data component.\n",
    "    with open(f'{data_path}/data', 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transform data step\n",
    "\n",
    "def transform_data(data_path: InputPath(str), \n",
    "              transform_data_path: OutputPath(str)):\n",
    "    \n",
    "    # install the necessary libraries\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','scipy'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'geopy'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'numpy'])\n",
    "    \n",
    "    # import Libraries\n",
    "    import os, pickle;\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import geopy\n",
    "\n",
    "    \n",
    "    # load data from data_path\n",
    "    with open(f'{data_path}/data', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    # 8. Change ontime colunm to 1 for ontime and 0 for delay\n",
    "    data.ontime.replace({'G':1, np.NaN:0}, inplace=True)\n",
    "\n",
    "    # 3. Replace NaN with unknown for vechicle type\n",
    "    data.vehicleType.replace(np.NaN, 'unknown', inplace=True)\n",
    "    \n",
    "    # 10. Filling NaN in Transportation distance with difference in lon/lot of origin and destination\n",
    "    from geopy import distance\n",
    "\n",
    "    geodistance_km = []\n",
    "    for row in data.itertuples(index=False):\n",
    "        geodistance_km.append(distance.distance(row.Org_lat_lon, row.Des_lat_lon).km)\n",
    "\n",
    "    data['geodistaince_km']=geodistance_km\n",
    "\n",
    "    # Replace NaN row in 'TRANSPORTATION_DISTANCE_IN_KM' with geodisatnce values\n",
    "    data.TRANSPORTATION_DISTANCE_IN_KM.fillna(data.geodistaince_km, inplace=True)\n",
    "    \n",
    "    #11. Create the expected travel time in hours\n",
    "    data['expected_travelhours']=(data.Planned_ETA-data.trip_start_date).astype('timedelta64[h]')\n",
    "\n",
    "    # There are negative travel hours. I replace them with 0 hour.\n",
    "    data.expected_travelhours[data.expected_travelhours<0]=0\n",
    "    data.expected_travelhours.sort_values()\n",
    "\n",
    "    # 6784 row can be dropped, because it looks like an outlier.\n",
    "    data.drop(index=6784,axis=0,inplace=True)\n",
    "    \n",
    "    # Based on the consideration above, I use the following the first 9 columes as input features. The last one is the target.\n",
    "    data_use = data[['Market/Regular ','OriginLocation_Code','DestinationLocation_Code',\n",
    "                 'TRANSPORTATION_DISTANCE_IN_KM','vehicleType',\n",
    "                 'customerID','supplierID','Material Shipped','ontime']] #'expected_travelhours',\n",
    "    # They are not many som just trop them.\n",
    "    data_use.dropna(axis=0,inplace=True)\n",
    "\n",
    "    # Fix these columns\n",
    "    data_use['OriginLocation_Code'] = data_use['OriginLocation_Code'].apply(str)\n",
    "    data_use['DestinationLocation_Code'] = data_use['DestinationLocation_Code'].apply(str)\n",
    "    data_use['supplierID'] = data_use['supplierID'].apply(str)\n",
    "\n",
    "    \n",
    "    #creating the transform_data_path\n",
    "    os.makedirs(transform_data_path, exist_ok = True)\n",
    "    \n",
    "    #Save data as a pickle file to be used by the feature_engineering component.\n",
    "    with open(f'{transform_data_path}/data', 'wb') as f:\n",
    "        pickle.dump(data_use, f)\n",
    "    \n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature engineering step\n",
    "\n",
    "def feature_engineering(transform_data_path: InputPath(str), \n",
    "            feat_eng_path: OutputPath(str)):\n",
    "    \n",
    "    # install the necessary libraries\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','scikit-learn'])\n",
    "    \n",
    "  \n",
    "    \n",
    "    # import Library\n",
    "    import os, pickle;\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # loading the data\n",
    "    with open(f'{transform_data_path}/data', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        \n",
    "    # Take a copy for preproessing\n",
    "    df = data.copy()\n",
    "\n",
    "    # Make X and y\n",
    "    X = df.drop(columns='ontime', axis=1)\n",
    "    y = df['ontime']\n",
    "    \n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    encoder = OrdinalEncoder()\n",
    "    cat_columns = ['Market/Regular ','OriginLocation_Code','DestinationLocation_Code','vehicleType','customerID','supplierID','Material Shipped']\n",
    "    X[cat_columns] = encoder.fit_transform(X[cat_columns])\n",
    "    \n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=22, stratify=y)\n",
    "\n",
    "    # creating the feat_eng_path\n",
    "    os.makedirs(feat_eng_path, exist_ok = True)\n",
    "      \n",
    "    # save the train_test_split data as a pickle file to be used by the modeling component.\n",
    "    with open(f'{feat_eng_path}/split_data', 'wb') as f:\n",
    "        pickle.dump((X_train, X_test, y_train, y_test), f)\n",
    "    \n",
    "    return(print('Done!'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# xgboost modeling step\n",
    "\n",
    "def xgboost_modeling(feat_eng_path: InputPath(str), \n",
    "                     xgb_ensemble_path: OutputPath(str),\n",
    "                     mlpipeline_ui_metadata_path: OutputPath(str)):\n",
    "    \n",
    "    # install the necessary libraries\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','xgboost'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','scikit-learn'])\n",
    "    \n",
    "    # import Library\n",
    "    import os, json, pickle, joblib;\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from collections import namedtuple\n",
    "\n",
    "    #loading the split_data data\n",
    "    with open(f'{feat_eng_path}/split_data', 'rb') as f:\n",
    "        X_train, X_test, y_train, y_test = pickle.load(f)\n",
    "            \n",
    "    #creating the ensemble_path directory\n",
    "    os.makedirs(xgb_ensemble_path, exist_ok = True)\n",
    "    \n",
    "    # model initialization\n",
    "    xgb=XGBClassifier(scale_pos_weight=0.3627, \n",
    "                      max_depth=10, \n",
    "                      learning_rate=0.1043242, \n",
    "                      n_estimators=600, \n",
    "                      colsample_bylevel=0.8, \n",
    "                      reg_alpha=0.8,\n",
    "                      silent=True, \n",
    "                      metrics='auc', \n",
    "                      random_state=22)\n",
    "    \n",
    "    # fitting\n",
    "    xgb.fit(X_train,y_train,eval_set=[(X_train,y_train),(X_test, y_test)], early_stopping_rounds=50,verbose=50)\n",
    "    \n",
    "    # predict\n",
    "    xgb_pred = xgb.predict(X_test)\n",
    "    \n",
    "    #Save the predicted data as a pickle file to be used by the ensembling component.\n",
    "    with open(f'{xgb_ensemble_path}/xgb_pred', 'wb') as f:\n",
    "        pickle.dump(xgb_pred, f) \n",
    "    \n",
    "    # plot confusion_matrix\n",
    "    cm = confusion_matrix(y_test, xgb_pred)\n",
    "    vocab = list(np.unique(y_test))\n",
    "    \n",
    "    # confusion_matrix pair dataset \n",
    "    data = []\n",
    "    for target_index, target_row in enumerate(cm):\n",
    "        for predicted_index, count in enumerate(target_row):\n",
    "            data.append((vocab[target_index], vocab[predicted_index], count))\n",
    "    \n",
    "    # convert confusion_matrix pair dataset to dataframe\n",
    "    df = pd.DataFrame(data,columns=['target','predicted','count'])\n",
    "    \n",
    "    # change 'target', 'predicted' to integer strings\n",
    "    df[['target', 'predicted']] = (df[['target', 'predicted']].astype(int)).astype(str)\n",
    "    \n",
    "    #create kubeflow metric metadata for UI\n",
    "    metadata = {\n",
    "                \"outputs\": [\n",
    "                    {\n",
    "                        \"type\": \"confusion_matrix\",\n",
    "                        \"format\": \"csv\",\n",
    "                        \"schema\": [\n",
    "                            {\n",
    "                                \"name\": \"target\",\n",
    "                                \"type\": \"CATEGORY\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"name\": \"predicted\",\n",
    "                                \"type\": \"CATEGORY\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"name\": \"count\",\n",
    "                                \"type\": \"NUMBER\"\n",
    "                            }\n",
    "                        ],\n",
    "                        \"source\": df.to_csv(header=False, index=False),\n",
    "                        \"storage\": \"inline\",\n",
    "                        \"labels\": [\n",
    "                            \"0\",\n",
    "                            \"1\"\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "    \n",
    "    with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:\n",
    "        json.dump(metadata, metadata_file)\n",
    "\n",
    "    conf_m_result = namedtuple('conf_m_result', ['mlpipeline_ui_metadata'])\n",
    "    \n",
    "    return conf_m_result(json.dumps(metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightgbm modeling step\n",
    "\n",
    "def lightgbm_modeling(feat_eng_path: InputPath(str), \n",
    "                      lgbm_ensemble_path: OutputPath(str),\n",
    "                      mlpipeline_ui_metadata_path: OutputPath(str)):\n",
    "    \n",
    "    # install the necessary libraries\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','lightgbm'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','scikit-learn'])\n",
    "    \n",
    "    # import Library\n",
    "    import os, json, pickle;\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from lightgbm import LGBMClassifier\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from collections import namedtuple\n",
    "\n",
    "    #loading the new_feats data\n",
    "    with open(f'{feat_eng_path}/split_data', 'rb') as f:\n",
    "        X_train, X_test, y_train, y_test = pickle.load(f)\n",
    "        \n",
    "    \n",
    "    #creating the ensemble_path directory\n",
    "    os.makedirs(lgbm_ensemble_path, exist_ok = True)\n",
    "    \n",
    "    # model initialization\n",
    "    lgbm = LGBMClassifier(random_state=22,scale_pos_weight=0.362)\n",
    "\n",
    "    # fitting\n",
    "    lgbm.fit(X_train, y_train, categorical_feature = 'auto', eval_set=(X_test, y_test),feature_name='auto', verbose=0)\n",
    "    \n",
    "    # predict\n",
    "    lgbm_pred = lgbm.predict(X_test)\n",
    "    \n",
    "    #Save the predicted data as a pickle file to be used by the ensembling component.\n",
    "    with open(f'{lgbm_ensemble_path}/lgbm_pred', 'wb') as f:\n",
    "        pickle.dump((y_test, lgbm_pred), f)\n",
    "    \n",
    "    # plot confusion_matrix\n",
    "    cm = confusion_matrix(y_test, lgbm_pred)\n",
    "    vocab = list(np.unique(y_test))\n",
    "    \n",
    "    # confusion_matrix pair dataset \n",
    "    data = []\n",
    "    for target_index, target_row in enumerate(cm):\n",
    "        for predicted_index, count in enumerate(target_row):\n",
    "            data.append((vocab[target_index], vocab[predicted_index], count))\n",
    "    \n",
    "    # convert confusion_matrix pair dataset to dataframe\n",
    "    df = pd.DataFrame(data,columns=['target','predicted','count'])\n",
    "    \n",
    "    # change 'target', 'predicted' to integer strings\n",
    "    df[['target', 'predicted']] = (df[['target', 'predicted']].astype(int)).astype(str)\n",
    "    \n",
    "    # create kubeflow metric metadata for UI\n",
    "    metadata = {\n",
    "                \"outputs\": [\n",
    "                    {\n",
    "                        \"type\": \"confusion_matrix\",\n",
    "                        \"format\": \"csv\",\n",
    "                        \"schema\": [\n",
    "                            {\n",
    "                                \"name\": \"target\",\n",
    "                                \"type\": \"CATEGORY\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"name\": \"predicted\",\n",
    "                                \"type\": \"CATEGORY\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"name\": \"count\",\n",
    "                                \"type\": \"NUMBER\"\n",
    "                            }\n",
    "                        ],\n",
    "                        \"source\": df.to_csv(header=False, index=False),\n",
    "                        \"storage\": \"inline\",\n",
    "                        \"labels\": [\n",
    "                            \"0\",\n",
    "                            \"1\"\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "    \n",
    "    \n",
    "    \n",
    "    with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:\n",
    "        json.dump(metadata, metadata_file)\n",
    "\n",
    "    conf_m_result = namedtuple('conf_m_result', ['mlpipeline_ui_metadata'])\n",
    "    \n",
    "    return conf_m_result(json.dumps(metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensembling step\n",
    "\n",
    "def ensembling(lgbm_ensemble_path: InputPath(str),\n",
    "               xgb_ensemble_path: InputPath(str),\n",
    "              # cb_ensemble_path: InputPath(str),\n",
    "               mlpipeline_ui_metadata_path: OutputPath(str)) -> NamedTuple('conf_m_result', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    \n",
    "    # install the necessary libraries\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','scipy'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','scikit-learn'])\n",
    "    \n",
    "    # import Library\n",
    "    import os, json, pickle;\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from scipy import stats\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    #loading the new_feats data\n",
    "    with open(f'{lgbm_ensemble_path}/lgbm_pred', 'rb') as f:\n",
    "        (y_test, lgbm_pred) = pickle.load(f)\n",
    "    with open(f'{xgb_ensemble_path}/xgb_pred', 'rb') as g:\n",
    "        xgb_pred = pickle.load(g)\n",
    "#     with open(f'{cb_ensemble_path}/cb_pred', 'rb') as h:\n",
    "#         cb_pred = pickle.load(h)\n",
    "    \n",
    "    # create an array of all predictions\n",
    "    predictions = np.array([xgb_pred, lgbm_pred]) #cb_pred,\n",
    "    \n",
    "    # find the most frequent predicted value \n",
    "    pred_mode = stats.mode(predictions, axis=0)[0][0]\n",
    "    \n",
    "    # plot confusion_matrix\n",
    "    cm = confusion_matrix(y_test, pred_mode)\n",
    "    vocab = list(np.unique(y_test))\n",
    "    \n",
    "    # confusion_matrix pair dataset \n",
    "    data = []\n",
    "    for target_index, target_row in enumerate(cm):\n",
    "        for predicted_index, count in enumerate(target_row):\n",
    "            data.append((vocab[target_index], vocab[predicted_index], count))\n",
    "    \n",
    "    # convert confusion_matrix pair dataset to dataframe\n",
    "    df = pd.DataFrame(data,columns=['target','predicted','count'])\n",
    "    \n",
    "    # change 'target', 'predicted' to integer strings\n",
    "    df[['target', 'predicted']] = (df[['target', 'predicted']].astype(int)).astype(str)\n",
    "    \n",
    "    # create kubeflow metric metadata for UI\n",
    "    metadata = {\n",
    "                \"outputs\": [\n",
    "                    {\n",
    "                        \"type\": \"confusion_matrix\",\n",
    "                        \"format\": \"csv\",\n",
    "                        \"schema\": [\n",
    "                            {\n",
    "                                \"name\": \"target\",\n",
    "                                \"type\": \"CATEGORY\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"name\": \"predicted\",\n",
    "                                \"type\": \"CATEGORY\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"name\": \"count\",\n",
    "                                \"type\": \"NUMBER\"\n",
    "                            }\n",
    "                        ],\n",
    "                        \"source\": df.to_csv(header=False, index=False),\n",
    "                        \"storage\": \"inline\",\n",
    "                        \"labels\": [\n",
    "                            \"0\",\n",
    "                            \"1\"\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "    \n",
    "    with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:\n",
    "        json.dump(metadata, metadata_file)\n",
    "\n",
    "    conf_m_result = namedtuple('conf_m_result', ['mlpipeline_ui_metadata'])\n",
    "    \n",
    "    return conf_m_result(json.dumps(metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pipeline Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create light weight components\n",
    "load_op = comp.create_component_from_func(load_data,base_image=\"python:3.7.1\")\n",
    "transform_op = comp.create_component_from_func(transform_data,base_image=\"python:3.7.1\")\n",
    "feature_eng_op = comp.create_component_from_func(feature_engineering,base_image=\"python:3.7.1\")\n",
    "xgboost_modeling_op = comp.create_component_from_func(xgboost_modeling, base_image=\"python:3.7.1\")\n",
    "lightgbm_modeling_op = comp.create_component_from_func(lightgbm_modeling, base_image=\"python:3.7.1\")\n",
    "ensembling_op = comp.create_component_from_func(ensembling, base_image=\"python:3.7.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline\n",
    "@dsl.pipeline(name=\"logistics_service_analysis\", \n",
    "              description=\"Predicting Ontime Delivery.\")\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def logistics_service_analysis_pipeline(\n",
    "                             download_link: str,\n",
    "                             data_path: str,\n",
    "                             transform_data_path: str, \n",
    "                             feat_eng_data_path: str,\n",
    "                             xgb_ensemble_path:str,\n",
    "                             lgbm_ensemble_path:str\n",
    "                            ):\n",
    "\n",
    "    # Create load container.\n",
    "    load_container = load_op(download_link)\n",
    "    # Create transform container.\n",
    "    transform_container = transform_op(load_container.output)\n",
    "    # Create feature engineering container.\n",
    "    feature_eng_container = feature_eng_op(transform_container.output)\n",
    "    # Create xgboost modeling container.\n",
    "    xgb_modeling_container = xgboost_modeling_op(feature_eng_container.output)\n",
    "    # Create lightgbm modeling container.\n",
    "    lgbm_modeling_container = lightgbm_modeling_op(feature_eng_container.output)\n",
    "    # Create ensemble container.\n",
    "    ensembling_container = ensembling_op(lgbm_modeling_container.outputs[\"lgbm_ensemble\"], \\\n",
    "                                         xgb_modeling_container.outputs[\"xgb_ensemble\"])#, \\\n",
    "                                        # cb_modeling_container.outputs[\"cb_ensemble\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create client that would enable communication with the Pipelines API server \n",
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments\n",
    "download_link = 'https://github.com/saurabhpimpalkar8/Logistic_Analysis/blob/main/data/Delivery_truck_trip_data.xlsx?raw=true'\n",
    "data_path = \"data\"\n",
    "transform_data_path = \"tdp\"\n",
    "feat_eng_data_path = \"feat\"                         \n",
    "xgb_ensemble_path = \"xep\"\n",
    "lgbm_ensemble_path = \"lep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/264c6fa3-e698-4972-88b4-d06aad3dfcaf\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/db5745c9-a398-4ecb-9949-2ba82d8c27be\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline_func = logistics_service_analysis_pipeline\n",
    "\n",
    "experiment_name = 'logistics_service_analysis_pipeline'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\n",
    "             \"download_link\": download_link,\n",
    "             \"data_path\": data_path,\n",
    "             \"transform_data_path\": transform_data_path,\n",
    "             \"feat_eng_data_path\": feat_eng_data_path,\n",
    "             \"xgb_ensemble_path\": xgb_ensemble_path,\n",
    "             \"lgbm_ensemble_path\": lgbm_ensemble_path\n",
    "            }\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  \n",
    "  '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": true,
   "docker_image": "gcr.io/arrikto/jupyter-kale-py36@sha256:dd3f92ca66b46d247e4b9b6a9d84ffbb368646263c2e3909473c3b851f3fe198",
   "experiment": {
    "id": "",
    "name": ""
   },
   "experiment_name": "",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "",
   "pipeline_name": "",
   "snapshot_volumes": true,
   "steps_defaults": [
    "label:access-ml-pipeline:true",
    "label:access-rok:true"
   ],
   "volume_access_mode": "rwm",
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/home/jovyan",
     "name": "logisticanalysis-workspace-m98xh",
     "size": 5,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 57.840049,
   "end_time": "2021-12-14T16:10:42.581222",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-14T16:09:44.741173",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
